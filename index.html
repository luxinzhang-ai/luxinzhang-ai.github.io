<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Title -->
	<title>Luxin Zhang</title>

	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

	<!-- Bootstrap core CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
		integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
	<!-- https://fontawesome.com/cheatsheet -->
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css"
		integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
	<link rel="stylesheet" href="static/styles.css">

	<!-- Scripts -->
	<script type="text/javascript" src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>

	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-124898353-1"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag() { dataLayer.push(arguments); }
		gtag('js', new Date());
		gtag('config', 'UA-124898353-1');
	</script>

	<!-- Show more content -->
	<script type="text/javascript">
		function toggle_vis(id) {
			// var e = document.getElementById(id);
			var e = document.getElementsByClassName(id);
			var showText = document.getElementById("showText");
			for (var i = 0; i < e.length; i++) {
				if (e[i].style.display == "none") {
					e[i].style.display = "inline";
					showText.innerHTML = "[Show less]";
				} else {
					e[i].style.display = "none";
					showText.innerHTML = "[Show more]";
				}
			}
		}
	</script>

	<!-- Custom br space -->
	<style type="text/css">
		.brsmall {
			display: block;
			margin-bottom: 0.75em;
		}

		.brmedium {
			display: block;
			margin-bottom: 1em;
		}
	</style>
</head>

<body>
	<nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark">
		<div class="container">
			<a class="navbar-brand" href="#">Luxin Zhang</a>

			<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle">
				<span class="navbar-toggler-icon"></span>
			</button>

			<div class="collapse navbar-collapse" id="navbarToggle">
				<ul class="navbar-nav ml-auto">
					<li class="nav-item">
						<a class="nav-link" href="#">Home</a>
					</li>
					<li class="nav-item">
						<a class="nav-link" href="#Experience">Experience</a>
					</li>
					<li class="nav-item">
						<a class="nav-link" href="#Publications">Publications</a>
					</li>
				</ul>
			</div>
		</div>
	</nav>

	<div class="container" style="padding-top: 20px">
		<div class="row">
			<div class="col-md-3" , style="padding-top: 65px">
				<img class="img-responsive img-rounded" src="img/luxin.png" alt=""
					style="max-width: 240px; border:2px solid gray">
			</div>
			<div class="col-md-9" , style="padding-top: 65px">

				<h3>Luxin Zhang</h3>


				<p>
					I am a Member of Technical Staff at a video AI startup.
					Previously, I was a research engineer at Meta Super Intelligence Lab.
					Prior to that, I received my master's degree from
					<a target="_blank"
						href="https://www.ri.cmu.edu/education/academic-programs/master-of-science-computer-vision/">Master
						of Science in Computer Vision (MSCV)</a>
					program in
					<a target="_blank" href="https://www.ri.cmu.edu/">The Robotics Institute</a> at
					<a target="_blank" href="https://www.cmu.edu/">Carnegie Mellon University</a>.

					I completed my bachelor's study at
					<a target="_blank" href="https://sai.pku.edu.cn/znxyenglish/">Department of Intelligence
						Science</a>,
					<a target="_blank" href="https://eecs.pku.edu.cn/en/">School of EECS</a>,
					<a target="_blank" href="https://english.pku.edu.cn/">Peking University</a>.

					I am interested in Generative AI, Computer Vision and Machine Learning.
				</p>

				<br>

				<p>
					<a target="_blank" href="https://scholar.google.com/citations?user=RMfu8yEAAAAJ&hl=en">
						<font color="black"><i class="ai ai-google-scholar ai-lg"></i></font>
					</a>&nbsp;&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;
					<!-- <a target="_blank" href="https://github.com/lucinezhang"><font color="black"><i class="fab fa-github fa-lg"></i></font></a>&nbsp;&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp; -->
					<a target="_blank" href="https://www.linkedin.com/in/luxin-zhang-cmu/">
						<font color="black"><i class="fab fa-linkedin fa-lg"></i></font>
					</a>&nbsp;&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;
					Email:</span>&nbsp; <tt>luxinz[at]alumni.cmu.edu</tt>&nbsp;&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;
					<a href="static/CV_LUXIN_ZHANG.pdf">Curriculum Vitae</a>
				</p>

			</div>
		</div>
	</div><br><br>


	<!-- Education -->
	<!-- <div class="container">
		<h3 id="Education" style="padding-top: 80px; margin-top: -80px;">Educations</h3><hr>
		<table class="imgtable"><tr><td>
			<img src="img/cmu.png" alt="CMU" width="100px" height="100px" />&nbsp;&nbsp;&nbsp;&nbsp;</td>
			<td align="left">
				<p><i>Aug. 2018 - Dec. 2019</i> <br>
				<a target="_blank" href="https://www.scs.cmu.edu/">School of Computer Science</a>,
				<b><a target="_blank" href="https://www.cmu.edu/">Carnegie Mellon University</a></b><br>
				Master of Science in Computer Vision <br>
				</p>
		</td></tr></table>

		<table class="imgtable"><tr><td>
			<img src="img/pku.png" alt="Peking" width="100px" height="100px" />&nbsp;&nbsp;&nbsp;&nbsp;</td>
			<td align="left">
				<p><i>Sep. 2014 - Jul. 2018</i> <br>
				<a target="_blank" href="http://www.cis.pku.edu.cn/english_index.htm">Department of Computer Intelligence Science</a>,
				<b><a target="_blank" href="http://english.pku.edu.cn/">Peking University</a></b><br>
				Bachelor of Science <br>
				</p>
		</td></tr></table>
		<hr>

		<br>
	</div><br> -->

	<!-- Highlight -->
	<div class="container">
		<h4 id="" style="padding-top: 80px; margin-top: -80px;">Research Highlight</h4>
		<hr>

		<div class="row">
			<div class="twelve columns" style="margin-top: 0%">
				<video style="display: block; margin-left: auto; margin-right: auto; width: 50%; height: auto;" controls
					autoplay muted loop>
					<source src="img/moviegen.mp4" type="video/mp4">
					Your browser does not support the video tag.
				</video>
			</div>
		</div>
		<div align="center">
			<p style="margin-top: 20px; font-size: 20px;"><a target="_blank"
					href="https://ai.meta.com/research/movie-gen/">Meta Movie Gen</a>: the most advanced media
				foundation models.</p>
		</div>

		<br>
	</div>


	<!-- Research -->
	<!-- <div class="container">
		<h3 id="Research" style="padding-top: 80px; margin-top: -80px;">Research</h3><hr>
			<h4 style="padding-top: 80px; margin-top: -80px;">Modeling human attention for deep imitation learning</h4>
				<p>Time: <i>Jul. 2017 - Jun. 2018</i></p>
				<img src="img/seaquest.png" height="180px">
				<img src="img/breakout.png" height="180px">
				<img src="img/gaze_network.png" height="180px">
				<img src="img/action_network.png" height="150px">
				<br>
				<p>When an intelligent agent learns to imitate human visuomotor behaviors, it may benefit from knowing where the human is allocating visual attention, which can be inferred from their gaze. A wealth of information regarding intelligent decision making is conveyed by human gaze allocation; hence, exploiting such information has the potential to improve the agent's performance. With this motivation, we collect high-quality human action and gaze data while playing Atari games in a carefully controlled experimental setting. Using these data, we first train a deep neural network that can predict human gaze positions and visual attention with high accuracy (the gaze network) and then train another deep neural network to predict human actions (the policy network). Including the gaze predictions from the gaze network in the policy network significantly improves the action prediction accuracy. We conclude that it is feasible to learn human attention in the given visuomotor tasks, and that combining the learned attention model with imitation learning yields promising results.<br><br>

				My work was to model visual attention from human eye movement data using a deep learning approach. I helped conduct psychophysical experiments that collect high-precision human eye tracking data when playing video games, and designed a three-channel convolution-deconvolution deep neural network, that simultaneously takes game image frames, motion information (optical flow), and image saliency information to predict where the human would allocate visual attention when playing the game. Eventually I obtained a deep neural network model that can predict human visual attention with high precision. We also experimented on using this visual attention model to facilitate the learning process of deep reinforcement learning and deep imitation learning algorithms.
				</p><hr>

			<h4 style="padding-top: 80px; margin-top: -80px;">Text effects transfer</h4>
				<p>Time: <i>Mar. 2017 - Jun. 2017</i></p>
				<img src="img/text_transfer.png" height="180px">
				<br>
				<p>Text effects transfer is a pretty novel research area. We studied the problem of transferring the text styles from source stylized image to target text image, that is, given a source stylized image S' and the target text image T, then automatically generates the target stylized image T' with the special effects as in S'. I tried different image segmentation methods to create a text mask for the stylized image S', like KNN clustering based on pixels' feature vectors and level set segmentation based on shape priors.
				</p><hr>

			<h4 style="padding-top: 80px; margin-top: -80px;">Cultural heritage protection based on virtual reality</h4>
				<p>Time: <i>Mar. 2016 - Jun. 2016</i></p>
				<br>
				<p>I participated in an interdisciplinary project between computer vision and archaeological conservation, in which we attempted to repair the faces of the Buddha of Longmen Grottoes using archived photos and display them on a virtual reality (VR) system. I was responsible for designing the VR user interface based on gesture recognition to enable the visitors to instruct the system to display the images, show text descriptions, or start the voice guide. I used Hidden Markov Model to implement a three-class gesture recognition, achieving an accuracy of 96% which met the requirements of the project.
				</p><hr>

		<br>
	</div><br> -->


	<!-- Publications -->
	<div class="container">
		<h4 id="Publications" style="padding-top: 80px; margin-top: -80px;">Publications</h4>
		<hr>

		<div class="row">
			<div class="col-md-9">
				<b>The Llama 4 Herd: The Beginning of A New Era of Natively Multimodal AI Innovation</b><br>
				<b>Core Contributor</b>, Multimodal Generation.<br>
				<b>Meta Technical Report, 2025</b><br>
				<a target="_blank" href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/">
					<small>[Blog]</small></a>
			</div>
		</div><br>

		<div class="row">
			<div class="col-md-9">
				<b>Movie Gen: A Cast of Media Foundation Models</b><br>
				<b>Core Contributor</b>, The Movie Gen Team.<br>
				<b>Meta Technical Report, 2024</b><br>
				<a target="_blank" href="https://ai.meta.com/static-resource/movie-gen-research-paper">
					<small>[PDF]</small></a>
				<a target="_blank" href="https://ai.meta.com/research/movie-gen/"> <small>[Blog]</small></a>
				<a target="_blank" href="https://www.youtube.com/watch?v=FHSSx4dUs7E"> <small>[Video]</small></a>
				<a target="_blank" href="https://github.com/facebookresearch/MovieGenBench"> <small>[Bench]</small></a>
			</div>
		</div><br>

		<div class="row">
			<div class="col-md-9">
				<b>MoCha: Towards Movie-Grade Talking Character Generation</b><br>
				Cong Wei, Bo Sun, Haoyu Ma, Ji Hou, Felix Juefei-Xu, Zecheng He, Xiaoliang Dai, <b>Luxin Zhang</b>,
				Kunpeng Li, Tingbo Hou, Animesh Sinha, Peter Vajda, Wenhu Chen<br>
				<b>NeurIPS 2025</b><br>
				<a target="_blank" href="https://arxiv.org/pdf/2503.23307">
					<small>[PDF]</small></a>
				<a target="_blank" href="https://congwei1230.github.io/MoCha/"> <small>[Project]</small></a>
			</div>
		</div><br>

		<div class="row">
			<div class="col-md-9">
				<b>Animated Stickers: Bringing Stickers to Life with Video Diffusion</b><br>
				David Yan, Winnie Zhang, <b>Luxin Zhang</b>, Anmol Kalia, Dingkang Wang, Ankit Ramchandani, Miao Liu,
				Albert Pumarola, Edgar Schoenfeld, Elliot Blanchard, Krishna Narni, Yaqiao Luo, Lawrence Chen, Guan
				Pang, Ali Thabet, Peter Vajda, Amy Bearman, Licheng Yu<br>
				<b>arXiv Preprint 2024</b><br>
				<a target="_blank" href="https://arxiv.org/pdf/2402.06088"> <small>[PDF]</small></a>
			</div>
		</div><br>

		<div class="row">
			<div class="col-md-9">
				<b>AVID: Any-Length Video Inpainting with Diffusion Model</b><br>
				Zhixing Zhang, Bichen Wu, Xiaoyan Wang, Yaqiao Luo, <b>Luxin Zhang</b>, Yinan Zhao, Peter Vajda,
				Dimitris Metaxas, Licheng Yu <br>
				<b>CVPR 2024</b><br>
				<a target="_blank"
					href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_AVID_Any-Length_Video_Inpainting_with_Diffusion_Model_CVPR_2024_paper.pdf">
					<small>[PDF]</small></a>
				<a target="_blank" href="https://zhang-zx.github.io/AVID/"> <small>[Project]</small></a>
				<a target="_blank" href="https://github.com/zhang-zx/AVID"> <small>[Code]</small></a>
			</div>
		</div><br>

		<div class="row">
			<div class="col-md-9">
				<b>
					<font color="black">Cloth Region Segmentation for Robust Grasp Selection</font>
				</b><br>
				Jianing Qian, Thomas Weng, <b>Luxin Zhang</b>, Brian Okorn, David Held<br>
				<b>IROS 2020</b><br>
				<a target="_blank" href="https://arxiv.org/pdf/2008.05626"> <small>[PDF]</small></a>
			</div>
		</div><br>

		<div class="row">
			<div class="col-md-9">
				<b>
					<font color="black">Atari-head: Atari Human Eye-Tracking and Demonstration Dataset</font>
				</b><br>
				Ruohan Zhang, Calen Walshe, Zhuode Liu, Lin Guan, Karl Muller, Jake Whritner, <b>Luxin Zhang</b>, Mary
				Hayhoe, Dana Ballard<br>
				<b>AAAI 2020</b><br>
				<a target="_blank" href="https://ojs.aaai.org/index.php/AAAI/article/download/6161/6017">
					<small>[PDF]</small></a>
			</div>
		</div><br>

		<div class="row">
			<div class="col-md-9">
				<b>
					<font color="black">Modelling complex perception-action choices</font>
				</b><br>
				Ruohan Zhang, Jake Whritner, Zhuode Liu, <b>Luxin Zhang</b>, Karl Muller, Mary Hayhoe, Dana Ballard<br>
				<b>Journal of Vision 2018</b><br>
				<a target="_blank" href="https://jov.arvojournals.org/article.aspx?articleid=2699524">
					<small>[Link]</small></a>
			</div>
		</div><br>

		<div class="row">
			<div class="col-md-9">
				<b>
					<font color="black">AGIL: Learning Attention from Human for Visuomotor Tasks</font>
				</b><br>
				Ruohan Zhang, Zhuode Liu, <b>Luxin Zhang</b>, Jake A Whritner, Karl S Muller, Mary M Hayhoe, Dana H
				Ballard<br>
				<b>ECCV 2018</b><br>
				<a target="_blank"
					href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ruohan_Zhang_AGIL_Learning_Attention_ECCV_2018_paper.pdf">
					<small>[PDF]</small></a>
			</div>
		</div><br>

		<div class="row">
			<div class="col-md-9">
				<b>
					<font color="black">Learning Attention Model from Human for Visuomotor Tasks</font>
				</b><br>
				<b>Luxin Zhang</b>, Ruohan Zhang, Zhuode Liu, Mary Hayhoe, Dana Ballard<br>
				<b>AAAI 2018</b><br>
				<a target="_blank" href="https://ojs.aaai.org/index.php/AAAI/article/view/12147/12006">
					<small>[PDF]</small></a>
				<a target="_blank" href="https://www.youtube.com/watch?v=-zTX9VFSFME"> <small>[Video]</small></a>
			</div>
		</div><br>

		<div class="row">
			<div class="col-md-9">
				<b>
					<font color="black">Visual Attention Guided Deep Imitation Learning</font>
				</b><br>
				Ruohan Zhang, Zhuode Liu, <b>Luxin Zhang</b>, Karl S Muller Mary M Hayhoe, Dana H Ballard<br>
				spotlight paper,
				<b>NIPS 2017 Cognitively Informed Artificial Intelligence Workshop</b><br>
				<a target="_blank"
					href="https://pdfs.semanticscholar.org/8d95/238dbe37e3dae4f6d81aea6663e143e02a62.pdf">
					<small>[PDF]</small></a>
			</div>
		</div>
		<br>
	</div><br>

	<!-- Experience -->
	<div class="container">
		<h4 id="Experience" style="padding-top: 80px; margin-top: -80px;">Experience</h4>
		<hr>

		<table class="imgtable">
			<tr>
				<td>
					<img src="img/startup.jpeg" alt="Startup" width="70px" height="70px" />&nbsp;&nbsp;&nbsp;&nbsp;
				</td>
				<td align="left">
					<br>
					<p><i>Jun. 2025 - present</i> <br>
						<b> Video AI Startup </b> <br>
						Member of Technical Staff<br>
					</p>
				</td>
			</tr>
		</table>

		<table class="imgtable">
			<tr>
				<td>
					<img src="img/meta.png" alt="Meta" width="70px" height="70px" />&nbsp;&nbsp;&nbsp;&nbsp;
				</td>
				<td align="left">
					<br>
					<p><i>Feb. 2023 - Jun. 2025</i> <br>
						<b> Meta, Super Intelligence Lab </b> <br>
						Research Engineer<br>
					</p>
				</td>
			</tr>
		</table>

		<table class="imgtable">
			<tr>
				<td>
					<img src="img/meta.png" alt="Meta" width="70px" height="70px" />&nbsp;&nbsp;&nbsp;&nbsp;
				</td>
				<td align="left">
					<br>
					<p><i>Mar. 2020 - Feb. 2023</i> <br>
						<b> Meta, Reality Lab </b> <br>
						Software Engineer<br>
					</p>
				</td>
			</tr>
		</table>

		<table class="imgtable">
			<tr>
				<td>
					<img src="img/facebook.png" alt="Facebook" width="70px" height="70px" />&nbsp;&nbsp;&nbsp;&nbsp;
				</td>
				<td align="left">
					<br>
					<p><i>May. 2019 - Aug. 2019</i> <br>
						<b> Facebook </b> <br>
						Software Engineer Intern<br>
					</p>
				</td>
			</tr>
		</table>

		<table class="imgtable">
			<tr>
				<td>
					<img src="img/cmu.png" alt="CMU" width="70px" height="70px" />&nbsp;&nbsp;&nbsp;&nbsp;
				</td>
				<td align="left">
					<br>
					<p><i>Feb. 2019 - Dec. 2019</i> <br>
						<a target="_blank" href="https://r-pad.github.io/">The Robots Perceiving and Doing (RPAD)
							Lab</a>,
						<b><a target="_blank" href="https://www.cmu.edu/">Carnegie Mellon University</a></b><br>
						Research Assistant to Professor <a target="_blank" href="https://davheld.github.io/">David
							Held</a> <br>
					</p>
				</td>
			</tr>
		</table>

		<table class="imgtable">
			<tr>
				<td>
					<img src="img/microsoft.jpeg" alt="Microsoft" width="70px" height="70px" />&nbsp;&nbsp;&nbsp;&nbsp;
				</td>
				<td align="left">
					<br>
					<p><i>Sep. 2017 - Jan. 2018</i> <br>
						Big Data Mining Group,
						<b><a target="_blank" href="http://www.msra.cn/">Microsoft Research Asia</a></b><br>
						Research Engineer Intern
						Supervised by Dr. <a target="_blank" href="https://tellarin.com/borje/">Borje Karlsson</a>
					</p>
				</td>
			</tr>
		</table>

		<table class="imgtable">
			<tr>
				<td>
					<img src="img/utaustin.png" alt="UT-Austin" width="70px" height="70px" />&nbsp;&nbsp;&nbsp;&nbsp;
				</td>
				<td align="left">
					<br>
					<p><i>Jul. 2017 - Sep. 2017</i> <br>
						<a target="_blank" href="http://www.cs.utexas.edu/~dana/vrlab/">Vision, Cognition, and Action VR
							Lab</a>,
						<b><a target="_blank" href="https://www.utexas.edu/">The University of Texas at
								Austin</a></b><br>
						Research Assistant to Professor <a target="_blank" href="https://www.cs.utexas.edu/~dana/">Dana
							Ballard</a>.
					</p>
				</td>
			</tr>
		</table>

		<table class="imgtable">
			<tr>
				<td>
					<img src="img/pku.png" alt="Peking" width="70px" height="70px" />&nbsp;&nbsp;&nbsp;&nbsp;
				</td>
				<td align="left">
					<br>
					<p><i>Sep. 2016 - Sep. 2017</i> <br>
						<a target="_blank" href="http://39.96.165.147/struct.html">Spatial and Temporal Restoration,
							Understanding and Compression Team (STRUCT)</a>,
						<b><a target="_blank" href="http://english.pku.edu.cn/">Peking University</a></b><br>
						Research Assistant to Professor <a target="_blank"
							href="http://39.96.165.147/people/liujiaying.html">Jiaying Liu</a>
					</p>
				</td>
			</tr>
		</table>

		<br>
	</div>


	<!-- Projects -->
	<!-- <div class="container">
		<h3 id="Projects" style="padding-top: 80px; margin-top: -80px;">Projects</h3><hr>

			<h4 id="Project1" style="padding-top: 80px; margin-top: -80px;">Static and Dynamic Gesture Recognition</h4>
				<img src="img/static_gesture.png" height="180px">
				<img src="img/3DCNN.png" height="180px">
				<br><br>
				<p>We implemented static gesture recognition using a convolutional neural network, obtained an accuracy of 90% on <a target="_blank" href="http://www.idiap.ch/resource/gestures/">Sebastien Marcel Static Hand Posture Database</a> (6 categories). We also extended to use this gesture recognition model to control the Dino Jump game.<br>

				We also implemented dynamic gesture recognition using a two-stream 3D convolutional neural network, obtained an accuracy of 91% on <a target="_blank" href="http://lshao.staff.shef.ac.uk/data/SheffieldKinectGesture.htm">Sheffield KInect Gesture (SKIG) Dataset</a> (10 categories).
				</p>
				<a target="_blank" href="https://github.com/lucinezhang/GestureRecognition"><small>[Source Code]</small></a>

			<hr>

			<h4 id="Project2" style="padding-top: 80px; margin-top: -80px;">Text and Image Classification</h4>
				<img src="img/text_classify.png" height="180px">
				<img src="img/image_classify.png" height="180px">
				<br><br>
				<p>We implemented text classification using <i>scikit-learn</i>. Compared the performance of different classifiers (Naive Bayesian, SVM, SGD, Decision Tree, KNN, K-means), achieved 85% accuracy (9 categories).<br>

				We also implemented images classification using <i>Keras</i> on a subset of ImageNet, achieved 80% accuracy (19 categories). We designed a CNN which consists of 3 convolutional layers followed by pooling layers and one fully connected layer.
				</p>
				<a target="_blank" href="https://github.com/lucinezhang/Machine-Learning--Classification"> <small>[Source Code]</small></a>

			<hr>

			<h4 id="Project3" style="padding-top: 80px; margin-top: -80px;">Face Detection and Recognition</h4>
				<img src="img/face.png" height="180px">
				<br><br>
				<p>In this project, we detected faces in given images, matched the faces to examples in a given photo gallery and identified the person. Face detection and alignment processes are implemented in Dlib. Face recognition uses a deep learning model that is fine-tuned from <a target="_blank" href="https://arxiv.org/abs/1412.1265"><i>Deeply learned face representations are sparse, selective, and robust</i></a> (DeepID2+).
				</p>

			<hr>

			<h4 id="Project4" style="padding-top: 80px; margin-top: -80px;">Visualizing the Bank Marketing Data Set</h4>
				<img src="img/visualization1.png" height="180px">
				<img src="img/visualization2.png" height="180px">
				<br><br>
				<p>In this project, I developed a client, server and database system to visualize the <a target="_blank" href="https://archive.ics.uci.edu/ml/datasets/Bank+Marketing">Bank Marketing Data Set</a>, with an interactive interface that allows users to customize the visualization.
				</p>
				<a target="_blank" href="https://github.com/lucinezhang/Visualization-of-Bank-Marketing-Dataset"> <small>[Source Code]</small></a>

			<hr>

			<h4 id="Project5" style="padding-top: 80px; margin-top: -80px;">Design and Control Robots In Simulation</h4>
				<img src="img/robot1.png" height="180px">
				<img src="img/robot2.png" height="180px">
				<img src="img/robot3.png" height="180px">
				<br><br>
				<p>In this project, we designed a multi-robot system on Webots where a team of robots are instructed to perform a set of navigation and interaction tasks.
				</p>
				<a target="_blank" href="https://github.com/lucinezhang/RobotControlOnWebots"> <small>[Source Code]</small></a>

			<hr>

			<h4 id="Project6" style="padding-top: 80px; margin-top: -80px;">Blend Pictures Seamlessly with Poisson Image Editing
</h4>
				<img src="img/seamless.png" height="180px">
				<br><br>
				<p>We learnt the paper of <a target="_blank" href="http://www.cs.virginia.edu/~connelly/class/2014/comp_photo/proj2/poisson.pdf">Poisson Image Editing</a> and the source code of the exiting function in OpenCV, then implemented the algorithm using C++ and OpenCV in Visual Studio.
				</p>

			<hr>

			<h4 id="Project7" style="padding-top: 80px; margin-top: -80px;">Mine Sweeper</h4>
				<img src="img/sweeper1.png" height="180px">
				<img src="img/sweeper2.png" height="180px">
				<br><br>
				<p>We designed the game Mine Sweeper using C++. In our implementation, we provided time counting in game, saving and loading at any time on your turn, including 3-level difficulty with different board size. We also wrote a simple graphical user interface with EasyX.
				</p>
				<a target="_blank" href="https://github.com/lucinezhang/MineSweeper"><small>[Source Code]</small></a>

			<hr>

		<br>
	</div><br> -->


	<!-- Honor -->
	<!-- <div class="container">
		<h3 id="Honors" style="padding-top: 80px; margin-top: -80px;">Honors and Awards</h3><hr>
		<ul>
			<li><p><b>Outstanding Research Award</b>, Peking University, 2015~2016</p></li>
			<li><p><b>Wu Si Scholarship</b>, Peking University, 2015~2016</p></li>
			<li><p><b>Chang Fei Scholarship</b>, School of EECS, Peking University, 2016~2017</p></li>
		</ul><hr>

		<br>
	</div><br> -->

	<div class="container">
		<hr>
		<center>
			<!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=333&t=tt&d=nKR2t_3ZegMV5woZHR0IquZryOK5Za-lxAEe19P3uVE&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'>
			</script>
			<br><hr> -->
			<footer>
				<p>&copy; Luxin Zhang 2026</p>
			</footer>
		</center>
	</div>
	<!-- /container -->

	<!-- Bootstrap core JavaScript -->
	<!-- Placed at the end of the document so the pages load faster -->
	<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
		integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
		crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js"
		integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4"
		crossorigin="anonymous"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js"
		integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1"
		crossorigin="anonymous"></script>
</body>

</html>